import os
import re
import pandas as pd
from supabase import create_client
from dotenv import load_dotenv

def run_the_handshake():
    # AG FIX: Point to SCRATCH paths
    BASE_PATH = "/Users/josephlf/.gemini/antigravity/scratch"
    
    # INPUT 1: The Human Contacts (Andrew's Export)
    CONTACTS_FILE = os.path.join(BASE_PATH, "backend/Scout_Data_Artifacts/pilot_inputs/AndrewWestRegion_2025.csv")
    
    # INPUT 2: The Leads (Leads generated by Phase 31)
    # The user code referenced 'Jan6_Western_TimeTravel_Leads.csv' but we generated 'Time_Travel_HitList_2023.csv'
    LEADS_FILE = os.path.join(BASE_PATH, "Time_Travel_HitList_2023.csv")
    
    OUTPUT_FILE = os.path.join(BASE_PATH, "backend/Scout_Data_Artifacts/Leads_With_Human_Contacts.csv")

    load_dotenv(os.path.join(BASE_PATH, ".env"))
    supabase_url = os.getenv("SUPABASE_URL", "").strip()
    supabase_key = (os.getenv("SUPABASE_SERVICE_KEY", "") or os.getenv("SUPABASE_KEY", "")).strip()

    if not supabase_url or not supabase_key:
        raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_KEY (or SUPABASE_KEY) in .env")

    print(f"Loading Contacts: {os.path.basename(CONTACTS_FILE)}")
    if not os.path.exists(CONTACTS_FILE):
        raise FileNotFoundError(f"Missing {CONTACTS_FILE}")
        
    print(f"Loading Leads: {os.path.basename(LEADS_FILE)}")
    if not os.path.exists(LEADS_FILE):
        raise FileNotFoundError(f"Missing {LEADS_FILE}")

    df_contacts = pd.read_csv(CONTACTS_FILE, low_memory=False)
    df_leads = pd.read_csv(LEADS_FILE, low_memory=False)

    print(f"   Contacts: {len(df_contacts):,}")
    print(f"   Leads:    {len(df_leads):,}")

    # Minimal state normalization (expand if needed)
    STATE_MAP = {
        "CALIFORNIA":"CA","ARIZONA":"AZ","NEVADA":"NV","OREGON":"OR","WASHINGTON":"WA",
        "COLORADO":"CO","UTAH":"UT","IDAHO":"ID","MONTANA":"MT","WYOMING":"WY","NEW MEXICO":"NM","ALASKA":"AK","HAWAII":"HI"
    }

    def normalize_state(x):
        if pd.isna(x): return ""
        s = str(x).strip().upper()
        if len(s) == 2: return s
        s = re.sub(r"[^A-Z\s]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return STATE_MAP.get(s, s[:2] if len(s) >= 2 else "")

    CANONICAL_FIRMS = {
        "MERCER":"MERCER","AON":"AON","ALLIANT":"ALLIANT","GALLAGHER":"GALLAGHER","HUB":"HUB",
        "WTW":"WTW","WILLIS TOWERS WATSON":"WTW","WILLIS":"WILLIS","LOCKTON":"LOCKTON","USI":"USI","MARSH":"MARSH"
    }

    def normalize_firm(x):
        if pd.isna(x): return ""
        s = str(x).upper().strip()
        s = re.sub(r"[^A-Z0-9\s]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        noise = {"INC","LLC","CORP","CORPORATION","COMPANY","CO","GROUP","SERVICES","INSURANCE","RISK","SOLUTIONS"}
        tokens = [t for t in s.split() if t not in noise]
        s2 = " ".join(tokens)
        for k, v in CANONICAL_FIRMS.items():
            if k in s2:
                return v
        return s2

    # Update these if headers differ based on Scan Andrew Export result
    c_company = "Company Name"
    c_state = "Contact State"  # or Broker Office State?
    c_name = "Contact Full Name"
    c_email = "Contact Email"
    c_phone = "Contact Mobile Phone 1"
    c_title = "Contact Job Title"
    c_role = "Contact Job Role" 
    c_office = "Broker Office Name"
    c_npn = "NPN Number"

    # Normalize keys for matching
    print("Normalizing Keys...")
    df_contacts["match_firm"] = df_contacts[c_company].apply(normalize_firm)
    df_contacts["match_state"] = df_contacts[c_state].apply(normalize_state)

    # Leads Columns from detective output: 'PROVIDER_NAME_NORM' (broker_2021 equivalent), 'PROVIDER_STATE' (broker_state)
    # Need to map user script expectations to actual csv headers
    # Lead Headers: SPONSOR_NAME, SPONSOR_STATE, LIVES, PROVIDER_NAME_NORM, PROVIDER_STATE...
    
    # User Code used 'broker_2021' and 'broker_state'.
    # I should rename if needed or check existing. 
    # Let's map dynamically
    
    lead_firm_col = "PROVIDER_NAME_NORM" if "PROVIDER_NAME_NORM" in df_leads.columns else "broker_2021"
    lead_state_col = "PROVIDER_STATE" if "PROVIDER_STATE" in df_leads.columns else "broker_state"
    
    df_leads["match_firm"] = df_leads[lead_firm_col].apply(normalize_firm)
    df_leads["match_state"] = df_leads[lead_state_col].apply(normalize_state)

    print("Executing The Handshake (Merging Leads <-> Humans)...")
    merged = pd.merge(df_leads, df_contacts, on=["match_firm", "match_state"], how="inner")
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    if not merged.empty:
        # Select output columns
        out_cols = [
            "SPONSOR_NAME", "LIVES", lead_firm_col, lead_state_col, 
            c_name, c_email, c_phone, c_title, c_company
        ]
        # Handle case differences in columns
        final_cols = []
        for c in out_cols:
            if c in merged.columns: final_cols.append(c)
        
        merged[final_cols].to_csv(OUTPUT_FILE, index=False)
        print(f"SUCCESS: Generated {len(merged):,} Broker-Plan Pairs.")
        print(f"Saved to: {OUTPUT_FILE}")
    else:
        print("WARNING: No matches found in Handshake.")

    # Ingest contacts into Supabase
    print("Ingesting Contacts to Supabase (Upsert)...")
    supabase = create_client(supabase_url, supabase_key)

    payload = []
    for _, row in df_contacts.iterrows():
        email_val = str(row.get(c_email, "")).strip()
        if not email_val or email_val.lower() == "nan":
            email_val = None

        payload.append({
            "contact_name": str(row.get(c_name, "")).strip(),
            "email": email_val,
            "phone": str(row.get(c_phone, "")).strip(),
            "job_title": str(row.get(c_title, "")).strip(),
            "job_role": str(row.get(c_role, "")).strip() if c_role else "",
            "npn": str(row.get(c_npn, "")).strip() if c_npn else "",
            "firm_name_raw": str(row.get(c_company, "")).strip(),
            "firm_norm": normalize_firm(row.get(c_company, "")),
            "office_name": str(row.get(c_office, "")).strip() if c_office else "",
            "state_raw": str(row.get(c_state, "")).strip(),
            "state_norm": normalize_state(row.get(c_state, "")),
            "source_file": os.path.basename(CONTACTS_FILE)
        })

    # Upsert only rows with email present (safe, avoids conflict edge cases)
    valid_payload = [p for p in payload if p["email"] is not None]

    chunk_size = 500
    for i in range(0, len(valid_payload), chunk_size):
        batch = valid_payload[i:i + chunk_size]
        try:
             supabase.table("gold_broker_contacts").upsert(batch, on_conflict="email").execute()
             print(f"Upserted contacts: {min(i + chunk_size, len(valid_payload)):,}/{len(valid_payload):,}")
        except Exception as e:
            # Table might not exist yet?
            print(f"Error Upserting Batch: {e}")
            break

    print("Handshake complete.")

if __name__ == "__main__":
    run_the_handshake()
